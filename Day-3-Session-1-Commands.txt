
scala> var list = 1 to 1000000

scala> val listRdd = sc.parallelize(list)
listRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26

scala> listRdd.getNumPartitions
res1: Int = 12

scala> val count=listRdd.filter(_ % 13 == 0).collect.size
count: Int = 76923

scala> val count=listRdd.filter(_ % 13 == 0).collect.size
count: Int = 76923

scala> val count=listRdd.filter(_ % 13 == 0).collect.size
count: Int = 76923

scala> listRdd.cache
res2: listRdd.type = ParallelCollectionRDD[0] at parallelize at <console>:26

scala> val count=listRdd.filter(_ % 13 == 0).collect.size
count: Int = 76923

scala> val count=listRdd.filter(_ % 13 == 0).collect.size
count: Int = 76923

scala> var truckJsonFile = sc.textFile("truck_info.json")
truckJsonFile: org.apache.spark.rdd.RDD[String] = truck_info.json MapPartitionsRDD[7] at textFile at <console>:24


scala> var bus=truckJsonFile.filter(_.contains("Bus"))
bus: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at filter at <console>:25

scala> bus.count
res4: Long = 4194

scala> truckJsonFile.count
res5: Long = 12608

scala> var car=truckJsonFile.filter(_.contains("Car"))
car: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at filter at <console>:25

scala> car.count
res6: Long = 4237

scala> var truck=truckJsonFile.filter(_.contains("Truck"))
truck: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at filter at <console>:25

scala> truck.count
res7: Long = 4177

scala> 4177+4237+4194
res8: Int = 12608


scala> bus.collect

scala> truckJsonFile.cache
res11: org.apache.spark.rdd.RDD[String] = truck_info.json MapPartitionsRDD[7] at textFile at <console>:24

scala> truckJsonFile.filter(_.contains("Bus")).filter(_.contains("\"speed\":52")).count
res12: Long = 21

scala> truckJsonFile.filter(_.contains("Bus")).filter(_.contains("\"speed\":52")).saveAs
saveAsObjectFile   saveAsTextFile

scala> truckJsonFile.filter(_.contains("Bus")).filter(_.contains("\"speed\":52")).saveAsTextFile("bus-speed-52")


scala> var truckInfoDF=spark.read.json("truck_info.json")
truckInfoDF: org.apache.spark.sql.DataFrame = [location: struct<lat: double, lon: double>, sensor1: double ... 5 more fields]

scala> truckInfoDF.printSchema
root
 |-- location: struct (nullable = true)
 |    |-- lat: double (nullable = true)
 |    |-- lon: double (nullable = true)
 |-- sensor1: double (nullable = true)
 |-- sensor2: double (nullable = true)
 |-- sensor3: boolean (nullable = true)
 |-- speed: long (nullable = true)
 |-- vehicleId: string (nullable = true)
 |-- vehicleType: string (nullable = true)


scala> truckInfoDF.show(5)

scala> truckInfoDF.createOrReplaceTempView("truck")

scala> spark.sql("show tables").show
21/07/10 10:31:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
21/07/10 10:31:09 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/07/10 10:31:09 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
|        |    truck|       true|
+--------+---------+-----------+


scala> spark.sql("select * from truck where speed >= 50 and speed <=60").show(10)

scala> spark.sql("select * from truck where speed >= 50 and speed <=60 and vehicleType='Bus'").show(10)

scala> spark.sql("select vehicleType, avg(sensor1), avg(sensor2) from truck group by vehicleType").show


